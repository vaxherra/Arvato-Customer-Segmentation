{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `AZDIAS` + `CUSTOMERS` dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 00. Importing packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "try:\n",
    "    from tqdm import tqdm\n",
    "except:\n",
    "    !pip install tqdm\n",
    "    from tqdm import tqdm    \n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "### my utils\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from eda_utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01. Loading datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customers = pd.read_csv(\"../arvato_data_processed/customers.csv\", low_memory = False, index_col = 'LNR')\n",
    "azdias = pd.read_csv(\"../arvato_data_processed/azdias.csv\", low_memory = False, index_col = 'LNR')\n",
    "\n",
    "customers_feature_types =  pd.read_csv(\"../arvato_data_processed/customers_feature_types.csv\", low_memory = False,index_col='Attribute' )\n",
    "azdias_feature_types =  pd.read_csv(\"../arvato_data_processed/azdias_feature_types.csv\", low_memory = False,index_col='Attribute' )\n",
    "\n",
    "print(customers_feature_types.shape,azdias_feature_types.shape)\n",
    "\n",
    "customers_features = set(customers_feature_types['Attribute.1'])\n",
    "azdias_features = set(azdias_feature_types['Attribute.1']) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02. Common features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_features=  customers_features.intersection(azdias_features)\n",
    "len(common_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 330 features in common, let's see what features are missing. We know from previous notebooks that `customers` initially had 3 more features than `azdias` dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customers_features.difference(azdias_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "azdias_features.difference(customers_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, after applying filtering for missingness in `azdias` and `customers` we have 330 common features.\n",
    "\n",
    "Three features present in `customers` are lacking entires in `azdias`:\n",
    "1. `CUSTOMER_GROUP`\n",
    "2. `ONLINE_PURCHASE`\n",
    "3. `PRODUCT_GROUP`\n",
    "\n",
    "These features are related to the customer-related information for which we **naturally don't have general population equivalent**: 1. what is the group of the customer (single/multiple buyer), 2. whether it bought something online from us, 3. what category of product was it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whereas customers lack two features present in general population:\n",
    "\n",
    "1. `KKK` - purchasing power\n",
    "2. `REGIOTYP` - neighbourhood, one of several possible classes: unknown, upper class, conservatives, upper middle class, middle class, lower middle class, traditional workers, marginal groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "azdias = azdias[common_features]\n",
    "customers = customers[common_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customers['dataset'] = 'customers'\n",
    "azdias['dataset'] = 'azdias'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joint_dataset = pd.concat([customers,azdias])\n",
    "assert customers.shape[0]+azdias.shape[0]==joint_dataset.shape[0]\n",
    "\n",
    "joint_dataset.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# free memory\n",
    "del customers\n",
    "del azdias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03. Features\n",
    "\n",
    "## 03.01. Standardizing features\n",
    "\n",
    "Upon inspecting metadata attributes in `DIAS Attributes - Values 2017.xlsx` I've found that there are a couple of features that need re-encoding:\n",
    "\n",
    "- `LP_FAMILIE_GROB` must be standardized. As initially there are multiple labels that refer to the same class!\n",
    "    - from documentation: \n",
    "     ```\n",
    "     1 -> single\n",
    "     2 - > couple\n",
    "     3,4,5 -> single parent\n",
    "     6,7,8 -> family\n",
    "     9,10,11 -> multiperson household\n",
    "     \n",
    "     ```\n",
    "     \n",
    "     \n",
    "I will replace all secondary values for a class with the first value of a class. Also, there seems to be a value `0` present, but for which we don't have any information, I'll replace `0` with NA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"Before standarization we have classes: {}\".format(np.unique(joint_dataset['LP_FAMILIE_GROB'].dropna())))\n",
    "\n",
    "\n",
    "joint_dataset['LP_FAMILIE_GROB'].replace(4,3,inplace=True)\n",
    "joint_dataset['LP_FAMILIE_GROB'].replace(5,3,inplace=True)\n",
    "\n",
    "joint_dataset['LP_FAMILIE_GROB'].replace(7,6,inplace=True)\n",
    "joint_dataset['LP_FAMILIE_GROB'].replace(8,6,inplace=True)\n",
    "\n",
    "joint_dataset['LP_FAMILIE_GROB'].replace(10,9,inplace=True)\n",
    "joint_dataset['LP_FAMILIE_GROB'].replace(11,9,inplace=True)\n",
    "\n",
    "joint_dataset['LP_FAMILIE_GROB'].replace(0,np.nan,inplace=True)\n",
    "\n",
    "\n",
    "print(\"After standarization we have classes: {}\".format(np.unique(joint_dataset['LP_FAMILIE_GROB'].dropna())))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `LP_STATUS_GROB` is a feature that encodes the same class with different labels. From the provided metadata in Exel file we know:\n",
    "   ```\n",
    "   1,2 - > low-income earners\n",
    "   3,4,5 -> average,erners\n",
    "   6,7 -> independants\n",
    "   8,9 -> houseowners\n",
    "   10 -> top earners\n",
    "   \n",
    "   ```\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Before standarization we have classes: {}\".format(np.unique(joint_dataset['LP_STATUS_GROB'].dropna())))\n",
    "\n",
    "joint_dataset['LP_STATUS_GROB'].replace(2,1,inplace=True)\n",
    "\n",
    "joint_dataset['LP_STATUS_GROB'].replace(4,3,inplace=True)\n",
    "joint_dataset['LP_STATUS_GROB'].replace(5,3,inplace=True)\n",
    "\n",
    "joint_dataset['LP_STATUS_GROB'].replace(7,6,inplace=True)\n",
    "\n",
    "joint_dataset['LP_STATUS_GROB'].replace(9,8,inplace=True)\n",
    "\n",
    "print(\"After standarization we have classes: {}\".format(np.unique(joint_dataset['LP_STATUS_GROB'].dropna())))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `PRAEGENDE_JUGENDJAHRE` could be simplified, thus I create a new feature: `PRAEGENDE_JUGENDJAHRE_SIM` to collapse a couple of classes together based on metadata:\n",
    "\n",
    "\n",
    "initial classes:\n",
    "```\n",
    "1\t40ies - war years (Mainstream, O+W)\n",
    "2\t40ies - reconstruction years (Avantgarde, O+W)\n",
    "3\t50ies - economic miracle (Mainstream, O+W)\n",
    "4\t50ies - milk bar / Individualisation (Avantgarde, O+W)\n",
    "5\t60ies - economic miracle (Mainstream, O+W)\n",
    "6\t60ies - generation 68 / student protestors (Avantgarde, W)\n",
    "7\t60ies - opponents to the building of the Wall (Avantgarde, O)\n",
    "8\t70ies - family orientation (Mainstream, O+W)\n",
    "9\t70ies - peace movement (Avantgarde, O+W)\n",
    "10\t80ies - Generation Golf (Mainstream, W)\n",
    "11\t80ies - ecological awareness (Avantgarde, W)\n",
    "12\t80ies - FDJ / communist party youth organisation (Mainstream, O)\n",
    "13\t80ies - Swords into ploughshares (Avantgarde, O)\n",
    "14\t90ies - digital media kids (Mainstream, O+W)\n",
    "15\t90ies - ecological awareness (Avantgarde, O+W)\n",
    "```\n",
    "simplified classes:\n",
    "\n",
    "```\n",
    "    1,2 -> 40ies\n",
    "    3,4 -> 50ies\n",
    "    5,6,7 -> 60ies\n",
    "    8,9 -> 70ies\n",
    "    10,11,12,13 -: 80ies\n",
    "    14,15 -> 90ies\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joint_dataset['PRAEGENDE_JUGENDJAHRE_SIM']= joint_dataset['PRAEGENDE_JUGENDJAHRE']\n",
    "\n",
    "print(\"Before standarization we have classes: {}\".format(np.unique(joint_dataset['PRAEGENDE_JUGENDJAHRE'].dropna())))\n",
    "\n",
    "joint_dataset['PRAEGENDE_JUGENDJAHRE_SIM'].replace(2,1,inplace=True)\n",
    "\n",
    "joint_dataset['PRAEGENDE_JUGENDJAHRE_SIM'].replace(4,3,inplace=True)\n",
    "\n",
    "joint_dataset['PRAEGENDE_JUGENDJAHRE_SIM'].replace(6,5,inplace=True)\n",
    "joint_dataset['PRAEGENDE_JUGENDJAHRE_SIM'].replace(7,5,inplace=True)\n",
    " \n",
    "joint_dataset['PRAEGENDE_JUGENDJAHRE_SIM'].replace(9,8,inplace=True)\n",
    "\n",
    "\n",
    "joint_dataset['PRAEGENDE_JUGENDJAHRE_SIM'].replace([11,12,13],10,inplace=True)\n",
    "\n",
    "\n",
    "joint_dataset['PRAEGENDE_JUGENDJAHRE_SIM'].replace(15,14,inplace=True)\n",
    "\n",
    "print(\"After standarization we have classes: {}\".format(np.unique(joint_dataset['PRAEGENDE_JUGENDJAHRE_SIM'].dropna())))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `CAMEO_DEUINTL_2015` can be also generalized/simplified: to `CAMEO_DEUINTL_2015_SIM`\n",
    "\n",
    "original annotations\n",
    "```\n",
    "11\tWealthy Households-Pre-Family Couples & Singles\n",
    "12\tWealthy Households-Young Couples With Children\n",
    "13\tWealthy Households-Families With School Age Children\n",
    "14\tWealthy Households-Older Families &  Mature Couples\n",
    "15\tWealthy Households-Elders In Retirement\n",
    "21\tProsperous Households-Pre-Family Couples & Singles\n",
    "22\tProsperous Households-Young Couples With Children\n",
    "23\tProsperous Households-Families With School Age Children\n",
    "24\tProsperous Households-Older Families & Mature Couples\n",
    "25\tProsperous Households-Elders In Retirement\n",
    "31\tComfortable Households-Pre-Family Couples & Singles\n",
    "32\tComfortable Households-Young Couples With Children\n",
    "33\tComfortable Households-Families With School Age Children\n",
    "34\tComfortable Households-Older Families & Mature Couples\n",
    "35\tComfortable Households-Elders In Retirement\n",
    "41\tLess Affluent Households-Pre-Family Couples & Singles\n",
    "42\tLess Affluent Households-Young Couples With Children\n",
    "43\tLess Affluent Households-Families With School Age Children\n",
    "44\tLess Affluent Households-Older Families & Mature Couples\n",
    "45\tLess Affluent Households-Elders In Retirement\n",
    "51\tPoorer Households-Pre-Family Couples & Singles\n",
    "52\tPoorer Households-Young Couples With Children\n",
    "53\tPoorer Households-Families With School Age Children\n",
    "54\tPoorer Households-Older Families & Mature Couples\n",
    "55\tPoorer Households-Elders In Retirement\n",
    "\n",
    "```\n",
    "\n",
    "simplified:\n",
    "\n",
    "```\n",
    "11,12,13,14,15 -> wealthy\n",
    "\n",
    "21,22,23,24,25 -> prosperous\n",
    "\n",
    "31,32,33,34,35 -> comfortable\n",
    "\n",
    "41,42,43,44,45 -> less affluent\n",
    "\n",
    "51,52,53,54,55 -> poorer\n",
    "\n",
    "```\n",
    "\n",
    "also, the `XX` is replaced as unknown:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joint_dataset['CAMEO_DEUINTL_2015_SIM'] = joint_dataset['CAMEO_DEUINTL_2015']\n",
    "\n",
    "\n",
    "joint_dataset['CAMEO_DEUINTL_2015'].replace('XX',np.nan,inplace=True)\n",
    " \n",
    "    \n",
    "print(\"Before standarization we have classes: {}\".format(np.unique(joint_dataset['CAMEO_DEUINTL_2015_SIM'].dropna())))\n",
    "\n",
    "joint_dataset['CAMEO_DEUINTL_2015_SIM'].replace(12,11,inplace=True)\n",
    "joint_dataset['CAMEO_DEUINTL_2015_SIM'].replace(13,11,inplace=True)\n",
    "joint_dataset['CAMEO_DEUINTL_2015_SIM'].replace(14,11,inplace=True)\n",
    "joint_dataset['CAMEO_DEUINTL_2015_SIM'].replace(15,11,inplace=True)\n",
    " \n",
    "joint_dataset['CAMEO_DEUINTL_2015_SIM'].replace(22,21,inplace=True)\n",
    "joint_dataset['CAMEO_DEUINTL_2015_SIM'].replace(23,21,inplace=True)\n",
    "joint_dataset['CAMEO_DEUINTL_2015_SIM'].replace(24,21,inplace=True)\n",
    "joint_dataset['CAMEO_DEUINTL_2015_SIM'].replace(25,21,inplace=True)\n",
    " \n",
    "joint_dataset['CAMEO_DEUINTL_2015_SIM'].replace(32,31,inplace=True)\n",
    "joint_dataset['CAMEO_DEUINTL_2015_SIM'].replace(33,31,inplace=True)\n",
    "joint_dataset['CAMEO_DEUINTL_2015_SIM'].replace(34,31,inplace=True)\n",
    "joint_dataset['CAMEO_DEUINTL_2015_SIM'].replace(35,31,inplace=True)\n",
    "\n",
    "joint_dataset['CAMEO_DEUINTL_2015_SIM'].replace(42,41,inplace=True)\n",
    "joint_dataset['CAMEO_DEUINTL_2015_SIM'].replace(43,41,inplace=True)\n",
    "joint_dataset['CAMEO_DEUINTL_2015_SIM'].replace(44,41,inplace=True)\n",
    "joint_dataset['CAMEO_DEUINTL_2015_SIM'].replace(45,41,inplace=True)\n",
    "\n",
    "joint_dataset['CAMEO_DEUINTL_2015_SIM'].replace(52,51,inplace=True)\n",
    "joint_dataset['CAMEO_DEUINTL_2015_SIM'].replace(53,51,inplace=True)\n",
    "joint_dataset['CAMEO_DEUINTL_2015_SIM'].replace(54,51,inplace=True)\n",
    "joint_dataset['CAMEO_DEUINTL_2015_SIM'].replace(55,51,inplace=True)\n",
    "    \n",
    "joint_dataset['CAMEO_DEUINTL_2015_SIM'].replace('XX',np.nan,inplace=True)    \n",
    "    \n",
    "print(\"After standarization we have classes: {}\".format(np.unique(joint_dataset['CAMEO_DEUINTL_2015_SIM'].dropna())))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(joint_dataset['CAMEO_DEUINTL_2015_SIM'].dropna())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- encode (check types)\n",
    "- impute\n",
    "- correlate\n",
    "- save independent?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 03.02. Encode Features\n",
    "\n",
    "joint dataset comprises of three types of features: \n",
    "\n",
    "1. numerical - will be left unchanged\n",
    "2. ordinal - will be encoded with label encoder\n",
    "3. categorical -> will be encoded with label encoder\n",
    "\n",
    "\n",
    "I will encode ordinal and categorical features as standardized integers. As seen above some classes comprise of high-value numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assert that the feature types in AZDIAS and Customers are the same for ALL\n",
    "common_features = pd.merge(customers_feature_types,azdias_feature_types,on='Attribute.1')\n",
    "assert common_features[common_features['Type_x']!=common_features[\"Type_y\"]].shape[0]==0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract feature names that **ARE NOT NUMERICAL** (that need label encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str_features = list(common_features[common_features['Type_x']!='numerical']['Attribute.1'])\n",
    "\n",
    "# two newly created features\n",
    "str_features.append('PRAEGENDE_JUGENDJAHRE_SIM')\n",
    "str_features.append('CAMEO_DEUINTL_2015_SIM')\n",
    "str_features = set(str_features)\n",
    "len(str_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joint_dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "LabelEncoders = { x: LabelEncoder() for x in str_features  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "empty_cells = pd.isnull(joint_dataset).sum()*100.0/joint_dataset.shape[0]\n",
    "empty_cells = empty_cells.sort_values(ascending=False)\n",
    "empty_cells[:50].plot(figsize=(20,3),kind='bar') # bar plot of first 50 most missing features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joint_dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for idx,feature in enumerate(str_features):\n",
    "    print(\"{}/{}: {}\".format(idx+1,len(str_features),feature ))\n",
    "    \n",
    "    not_na_mask =  joint_dataset[feature].notnull()\n",
    "    \n",
    "    joint_dataset[feature][not_na_mask] = LabelEncoders[feature].fit_transform(  joint_dataset[feature][not_na_mask]  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joint_dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "empty_cells = pd.isnull(joint_dataset).sum()*100.0/joint_dataset.shape[0]\n",
    "empty_cells = empty_cells.sort_values(ascending=False)\n",
    "empty_cells[:50].plot(figsize=(20,3),kind='bar') # bar plot of first 50 most missing features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 03.02 Impute features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joint_dataset.drop('dataset',inplace=True,axis='columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import sklearn \n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "print('The scikit-learn version is {}.'.format(sklearn.__version__))\n",
    "from sklearn.impute import IterativeImputer\n",
    "\n",
    "ie = IterativeImputer(n_nearest_features=3) # 10 is fine\n",
    "\n",
    "joint_dataset_imputed = ie.fit_transform(joint_dataset) # ~20min, returns numpy array\n",
    "joint_dataset_imputed_df = pd.DataFrame(joint_dataset_imputed, columns= joint_dataset.columns )\n",
    "joint_dataset_imputed_df.to_csv(\"../arvato_data_processed/joint_dataset_imputed.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joint_dataset_imputed_df =  pd.read_csv(\"../arvato_data_processed/joint_dataset_imputed.csv\", low_memory = False )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i,feature in enumerate(str_features):\n",
    "    print(\"{}/{} {}\".format(i+1,len(str_features), feature))\n",
    "    joint_dataset_imputed_df[feature] = joint_dataset_imputed_df[feature].round()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO\n",
    "if it was imputed, then compare the number of 'uniques', perhaps round"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "imputed_features_counts = {}\n",
    "imputed_features_counts['Attribute'] =[]\n",
    "imputed_features_counts['Before_imputation'] =[]\n",
    "imputed_features_counts['After_imputation'] =[]\n",
    "\n",
    "for idx,f in enumerate(str_features):\n",
    "    print(\"{}/{}: {}\".format(idx+1,len(str_features),f))\n",
    "    \n",
    "    imputed_features_counts['Attribute'].append(f)\n",
    "\n",
    "    uniq_before = len(np.unique(joint_dataset[f].dropna() ))\n",
    "    imputed_features_counts['Before_imputation'].append(uniq_before)\n",
    "    \n",
    "    uniq_after =   len(np.unique(joint_dataset_imputed_df[f].dropna() ))\n",
    "    imputed_features_counts['After_imputation'].append(uniq_after)\n",
    "    \n",
    "imputed_features_counts = pd.DataFrame.from_dict(imputed_features_counts)\n",
    "imputed_features_counts.sort_values('Before_imputation',ascending=False, inplace=True)\n",
    "imputed_features_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputed_features_counts[imputed_features_counts['Before_imputation']!=imputed_features_counts['After_imputation']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(joint_dataset['CAMEO_DEUINTL_2015_SIM'].dropna())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(joint_dataset_imputed_df['CAMEO_DEUINTL_2015_SIM'].dropna())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace seemingly new classes with NANs\n",
    "for feature in enumerate(str_features):\n",
    "    print(\"{}/{}: {}\".format(idx+1,len(str_features),f))\n",
    "    imputed_clusters = np.unique(joint_dataset_imputed_df[feature].dropna())\n",
    "    original_clusters = np.unique(joint_dataset[feature].dropna())\n",
    "\n",
    "\n",
    "    unobserved_classes = set(imputed_clusters).difference( set(original_clusters) )\n",
    "    for uclass in unobserved_classes:\n",
    "        joint_dataset_imputed_df[feature].replace(uclass,np.nan, inplace=True)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_amazonei_mxnet_p36",
   "language": "python",
   "name": "conda_amazonei_mxnet_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
